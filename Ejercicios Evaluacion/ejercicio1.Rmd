---
title: "Preprocesamiento y clasificación con conjunto de datos Titanic"
author: "Juan Gómez Romero"
date: "25 de febrero de 2021"
output:
  html_document:
      code_folding: "show"
      toc: true
      toc_depth: 2
      toc_float: true
      df_print: paged
---

Preprocesamiento de datos con el dataset [titanic](https://www.kaggle.com/c/titanic/).

> El hundimiento del Titanic es una de las tragedias marítimas más conocidas de la historia. El 15 de abril de 1912, durante su viaje inaugural, el Titanic se hundió después de chocar contra un iceberg. En el accidente murieron 1502 personas de las 2224 que habían embarcado, incluyendo pasajeros y tripulación. Una de las razones por las que no se encontraron más supervivientes fue la falta de espacio en los barcos salvavidas. Así, aunque la suerte sin duda sonrió a los supervivientes, también resultaron más favorecidos algunos grupos de personas, como las mujeres, los niños y los pasajeros de la clase superior.

**En este problema analizaremos qué tipos de personas tuvieron más probabilidades de sobrevivir. Para ello, aplicaremos técnicas de aprendizaje automático que nos permitirán predecir qué pasajeros sobrevivieron al hundimiento.**

En primer lugar, nos centraremos en el pre-procesamiento de los datos utilizando [tidyverse](https://www.tidyverse.org), una colección de paquetes de R para Ciencia de Datos. En el libro [*R for Data Science*](http://r4ds.had.co.nz) podemos encontrar documentación detallada sobre [tidyverse](https://www.tidyverse.org). A continuación pasaremos a estudiar la creación de modelos de clasificación con [<tt>caret</tt>](http://topepo.github.io/caret/). En el libro [*Applied Predictive Modeling*](https://link.springer.com/book/10.1007%2F978-1-4614-6849-3) (gratuito desde RedUGR) podemos encontrar documentación detallada sobre [<tt>caret</tt>](http://topepo.github.io/caret/).

# Lectura e inspección de datos

## Carga de datos

Comenzaremos utilizando el fichero [*train.csv*](https://www.kaggle.com/c/titanic/data) de Kaggle, donde encontramos los datos de 891 pasajeros y que utilizaremos para crear nuestro modelo de predicción.

Para lectura de datos, utilizaremos alguna de las variantes de la función [<tt>read</tt>](http://r4ds.had.co.nz/data-import.html). A continuación, podemos inspeccionar el contenido de la tabla de datos, que se almacena en formato [<tt>tibble</tt>](http://r4ds.had.co.nz/tibbles.html).

```{r cargar-tidyverse}
library(tidyverse)
data_raw <- read_csv('train.csv')
data_raw # str(data_raw) , glimpse(data_raw)
```

## Estado de los datos

Podemos identificar los valores perdidos de la tabla utilizando <tt>df_status</tt>, del paquete [<tt>funModeling</tt>](https://livebook.datascienceheroes.com/exploratory-data-analysis.html#dataset-health-status).

```{r}
library(funModeling)
df_status(data_raw)
```

Algunas observaciones interesantes:

-   Los valores de *PassengerId* y *Name* son únicos
-   Existen dos valores diferentes para *Survived*, que es nuestro objetivo de clasificación
-   No sobrevivieron 549 pasajeros (61.62%)
-   Aparecen numerosos valores perdidos (*na*) en las variables *Age* y *Cabin*

Parte de estas situaciones se pueden identificar y procesar directamente manipulando la tabla <tt>df_status</tt>:

```{r}
status <- df_status(data_raw)

## columnas con NAs
na_cols <- status %>%
  filter(p_na > 70) %>%
  select(variable)
## columnas con valores diferentes
dif_cols <- status %>%
  filter(unique > 0.8 * nrow(data_raw)) %>%
  select(variable)

## eliminar columnas
remove_cols <- bind_rows(
  list(na_cols, dif_cols)
)
data_reduced <- data_raw %>%
  select(-one_of(remove_cols$variable))

status_2 <- df_status(data_reduced)
```

```{r}
library(caret)
data <-
  data_raw %>%
  mutate(Survived = as.factor(ifelse(Survived == 1, 'Yes', 'No'))) %>%
  mutate(Pclass = as.factor(Pclass)) %>%
  mutate(Fare_Interval = as.factor(
    case_when(
      Fare >= 30 ~ 'More.than.30',
      Fare >= 20 & Fare < 30 ~ 'Between.20.30',
      Fare < 20 & Fare >= 10 ~ 'Between.10.20',
      Fare < 10 ~ 'Less.than.10'))) %>%
  select(Survived, Pclass, Sex, Fare_Interval)
```

## Aprendizaje de un modelo de clasificación utilizando 'Random Forest' (lo llamaremos modelo_rf1)

```{r}
rpartCtrl <- trainControl(classProbs = TRUE)
rpartParametersGrid <- expand.grid(.cp = c(0.01, 0.05)) # se pide que se prueben con la lista de valores .cp = {0.01, 0.05}
```

## Conjuntos de entrenamiento y validación

A continuación, se crean los conjuntos de entrenamiento y validación utilizando [<tt>createDataPartition</tt>](https://rdrr.io/rforge/caret/man/createDataPartition.html). En este caso utilizaremos 70% para entrenamiento (30% para validación) con selección aleatoria. El resultado de [<tt>createDataPartition</tt>](https://rdrr.io/rforge/caret/man/createDataPartition.html) es un vector (<tt>list = FALSE</tt>) con los números de fila que se han seleccionado para el entrenamiento.

```{r}
set.seed(0)
trainIndex_1 <- createDataPartition(data$Survived, p = .7, list = FALSE)
train_1 <- data[trainIndex, ]
val_1   <- data[-trainIndex, ]
```

También podemos calcular la curva ROC para las predicciones del modelo sobre los datos de validación utilizando el paquete [<tt>pROC</tt>](https://web.expasy.org/pROC/). El paquete incluye dos funcionalidades principales: <tt>auc</tt> para crear la curva y <tt>plot.roc</tt> para visualizarla.

El gráfico resultante muestra la curva ROC, el valor de corte para la probabilidad que optimiza el valor ROC y los valores de *specificity* y *sensitivity* resultantes.

```{r}
library(pROC)
rpartCtrl <- trainControl(verboseIter = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary, method = "cv", number = 10)
modelo_rf1 <- train(Survived ~ .,
                    data = train_1,
                    method = "rf",
                    metric = "ROC",
                    trControl = rpartCtrl,
                    tuneGrid = rpartParametersGrid)
predictionValidationProb_1 <- predict(modelo_rf1, val_1, type = "prob")
auc_1 <- roc(val_1$Survived, predictionValidationProb[["Yes"]])
roc_validation_1 <- plot.roc(auc_1,
                           ylim=c(0,1),
                           type = "S" ,
                           print.thres = TRUE,
                           main=paste('Validation AUC 1: ', round(auc_1$auc[[1]], 2)))
```

## Variación sobre modelo1 utilizando una partición de datos 80-20 y validación cruzada (modelo_rf2)

```{r}
set.seed(0)
trainIndex_2 <- createDataPartition(data$Survived, p = .8, list = FALSE)
train_2 <- data[trainIndex, ]
val_2 <- data[-trainIndex, ]
rpartCtrl_2 <- trainControl(verboseIter = TRUE, classProbs = TRUE, summaryFunction = twoClassSummary, method = "cv", number = 10)
modelo_rf2 <- train(Survived ~ .,
                    data = train_2,
                    method = "rf",
                    metric = "ROC",
                    trControl = rpartCtrl_2,
                    tuneGrid = rpartParametersGrid)
predictionValidationProb_2 <- predict(modelo_rf2, val_2, type = "prob")
auc_2 <- roc(val_2$Survived, predictionValidationProb[["Yes"]])
roc_validation_2 <- plot.roc(auc_2,
                           ylim=c(0,1),
                           type = "S" ,
                           print.thres = TRUE,
                           main=paste('Validation AUC 2: ', round(auc_2$auc[[1]], 2)))
```


## Comparación y selección del mejor modelo entre modelo_rf1, modelo_rf2 en términos de precisión y AUC (modelo_rf)

```{r}
roc.test(roc_validation_1, roc_validation_2)

plot.roc(auc_1, type = "S", col="#1c61b6")
lines.roc(auc_2, type = "S", col="#008600")

prediction_1 <- predict(modelo_rf1, val_1, type = "raw")
prediction_2 <- predict(modelo_rf2, val_2, type = "raw")

cm_train_1 <- confusionMatrix(prediction_1, val[["Survived"]])
cm_train_2 <- confusionMatrix(prediction_2, val[["Survived"]])
```


## Aprendizaje de modelo de clasificación utilizando redes neuronales - perceptrón multicapa y parámetros por defecto (modelo_rna)

```{r}
library(pROC)
modelo_rna <- train(Survived ~ .,
                    data = train_1,
                    method = "nnet",
                    metric = "ROC",
                    trControl = rpartCtrl)
predictionValidationProb_3 <- predict(modelo_rna, val_3, type = "prob")
auc_3 <- roc(val_3$Survived, predictionValidationProb[["Yes"]], levels = unique(val_3[["Survived"]]))
roc_validation_3 <- plot.roc(auc_3,
                           ylim=c(0,1),
                           type = "S" ,
                           print.thres = TRUE,
                           main=paste('Validation AUC 3: ', round(auc_3$auc[[1]], 2)))
```

## Mejora de modelo_rna mediante entrenamiento con rejilla de parámetros para los parámetros .size, .decay (modelo_rna_mejorado)
```{r}
library(pROC)
rpartParametersGrid <- expand.grid(.size=c(1, 20, 10),.decay=c(0, 0.01, 0.2))
modelo_rna_grid <- train(Survived ~ .,
                    data = train_1,
                    method = "nnet",
                    metric = "ROC",
                    trControl = rpartCtrl,
                    tuneGrid = rpartParametersGrid)
predictionValidationProb_4 <- predict(modelo_rna_grid, val_3, type = "prob")
auc_4 <- roc(val_4$Survived, predictionValidationProb[["Yes"]], levels = unique(val_4[["Survived"]]))
roc_validation_4 <- plot.roc(auc_4,
                           ylim=c(0,1),
                           type = "S" ,
                           print.thres = TRUE,
                           main=paste('Validation AUC 4: ', round(auc_4$auc[[1]], 2)))
```